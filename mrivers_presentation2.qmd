---
title: "NREL Geospatial Research Scientist Interview"
subtitle: "National Renewable Energy Laboratory"
author: "Marie Rivers"
date: "October 13, 2022"
format: 
  revealjs:
    theme: [default, presentation_styles.scss]
    slide-number: true
    preview-links: auto
    footer: <https://marierivers.github.io/wind_resource_temporal_variability/>
    title-slide-attributes:
      data-background-color: "#447099"
editor: visual
---

```{r}
#| include: false
library(tidyverse)
library(leaflet)
library(kableExtra)
```

```{python}
#| include: false
import h5pyd
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from pyproj import Proj
import dateutil
import folium
import plotly.graph_objects as go
```

```{python}
#| include: false
f = h5pyd.File("/nrel/wtk-us.h5", 'r', bucket="nrel-pds-hsds")

# access the windspeed_100m dataset
windspeed_100m_dset = f['windspeed_100m']

datetime_df = f["datetime"]
datetime_df = pd.DataFrame({"datetime": datetime_df[:]},index=range(0,datetime_df.shape[0]))
datetime_df['datetime'] = datetime_df['datetime'].apply(dateutil.parser.parse)

start_date = '2012-01-01'
end_date = '2013-01-01'
datetime_range = datetime_df.loc[(datetime_df.datetime >= start_date) & (datetime_df.datetime < end_date)].index

site_name = "Mount Washington"
site_coords = (44.27, -71.3)
```

```{python}
#| include: false
# This function finds the nearest x/y indices for a given lat/lon.
# Rather than fetching the entire coordinates database, which is 500+ MB, this
# uses the Proj4 library to find a nearby point and then converts to x/y indices

def indicesForCoord(f, lat_index, lon_index):
    dset_coords = f['coordinates']
    projstring = """+proj=lcc +lat_1=30 +lat_2=60 
                    +lat_0=38.47240422490422 +lon_0=-96.0 
                    +x_0=0 +y_0=0 +ellps=sphere 
                    +units=m +no_defs """
    projectLcc = Proj(projstring)
    origin_ll = reversed(dset_coords[0][0])  # Grab origin directly from database
    origin = projectLcc(*origin_ll)
    
    coords = (lon_index,lat_index)
    coords = projectLcc(*coords)
    delta = np.subtract(coords, origin)
    ij = [int(round(x/2000)) for x in delta]
    return tuple(reversed(ij))

nearest_site = indicesForCoord(f, site_coords[0], site_coords[1] )

print("y,x indices for", site_name, ": \t\t {}".format(nearest_site))
```

```{python}
#| include: false
nearest_site_coords = f["coordinates"][nearest_site[0]][nearest_site[1]]

site_map = folium.Map(location = site_coords, zoom_start = 10)
folium.Marker(site_coords, popup = site_name).add_to(site_map)
```

```{python}
#| include: false
tseries = windspeed_100m_dset[min(datetime_range):max(datetime_range)+1, nearest_site[0], nearest_site[1]]

windspeed_100m_df = pd.DataFrame(tseries, columns = ["windspeed_100m"], index = datetime_df.iloc[datetime_range,].datetime)

windspeed_100m_df["year"] = windspeed_100m_df.index.year
windspeed_100m_df["month"] = windspeed_100m_df.index.month
windspeed_100m_df["day"] = windspeed_100m_df.index.day
windspeed_100m_df["hour"] = windspeed_100m_df.index.hour
windspeed_100m_df["day_of_year"] = windspeed_100m_df.index.dayofyear
windspeed_100m_df["month_name"] = windspeed_100m_df.index.month_name()
```

```{python}
#| include: false
def quantile25(column):
    return column.quantile(0.25)
def quantile50(column):
    return column.quantile(0.50)
def quantile75(column):
    return column.quantile(0.75)

hourly_avg = windspeed_100m_df.groupby("hour")["windspeed_100m"].agg(["mean", "std", quantile25, quantile50, quantile75])
hourly_avg = hourly_avg.reset_index()
hourly_avg.head()

monthly_avg = windspeed_100m_df.groupby(["month", "month_name"])["windspeed_100m"].agg(["mean", "std", quantile25, quantile50, quantile75]).reset_index()

hourly_avg_by_month = windspeed_100m_df.groupby(["hour", "month"]).mean()
hourly_avg_by_month = hourly_avg_by_month.reset_index().pivot(index = "hour", columns = str("month"), values = "windspeed_100m")
hourly_avg_by_month.columns = hourly_avg_by_month.columns.astype(str)

hourly_std_by_month = windspeed_100m_df.groupby(["hour", "month"]).std()
hourly_std_by_month = hourly_std_by_month.reset_index().pivot(index = "hour", columns = str("month"), values = "windspeed_100m")
hourly_std_by_month.columns = hourly_avg_by_month.columns.astype(str)
```

```{python}
#| include: false
# 24 hour moving average
window_size_24hr = 24
windows_24hr = windspeed_100m_df.rolling(window_size_24hr)
moving_averages_24hr = windows_24hr.mean()

# 10 day moving average
window_size_10day = 240
windows_10day = windspeed_100m_df.rolling(window_size_10day)
moving_averages_10day = windows_10day.mean()

# 30 day moving average
window_size_30day = 720
windows_30day = windspeed_100m_df.rolling(window_size_30day)
moving_averages_30day = windows_30day.mean()
```

# Overview {.smaller}

[Professional background]{.text_block_overview} <br> <br> <br> [Wind variability technical task]{.text_block_overview .fragment} <br> <br> <br> [Improving usability of snow data through open-source tools]{.text_block_overview .fragment} <br> <br> <br> [Geospatial analysis of power outages]{.text_block_overview .fragment}

# Professional Background {background-color="#447099"}

## Environmental Engineering Consultant {.smaller}

-   Modeled drinking water distribution systems
-   Identified diurnal patterns in water use
-   Foretasted water demands and compared to future supply projections
-   Used diurnal and seasonal patters for extended period simulation models
-   Applied spatial analysis to infrastructure siting projects

::: footer
<https://marierivers.github.io/>
:::

## Environmental Data Science {.smaller}

::: columns
::: {.column width="70%"}
Obtained a Master of Environmental Data Science degree from UC Santa Barbara June 2022

Besides going to the beach and paddle boarding with my dog, I expanded my technical skills with

-   geospatial analysis
-   statistics
-   remote sensing
-   scientific programming with Python and R
-   data visualization
-   modeling environmental systems

...and fell in love with the concept of reproducible open science
:::

::: {.column width="30%"}
![](images/lyra_grad_pic.JPEG)
:::
:::

::: footer
<https://marierivers.github.io/>
:::

# Wind Resource Temporal Variability {background-color="#447099"}

## Data Workflow {.smaller}

::: columns
::: {.column width="60%"}
-   Explored diurnal and monthly variability of wind resources at Mount Washington in NH
-   Used the [NREL Wind Integration National Dataset (WIND) Toolkit](https://www.nrel.gov/grid/wind-toolkit.html)
-   Accessed data with the [h5pyd](https://github.com/HDFGroup/h5pyd) Python package and NREL [Highly Scalable Data Service (HSDS)](https://github.com/NREL/hsds-examples)
-   Subset the for the `windspeed_100m,` dataset and year 2012
-   Converted to pandas dataframe
-   Aggregated data by hourly and monthly groupings to calculate mean, standard deviation, and quartiles
:::

::: {.column width="40%"}
```{r}
leaflet_df_pres <- data.frame(location = c("Mount Washington"),
                         lat = c(as.numeric(44.27)),
                         lon = c(as.numeric(-71.3)))

leaflet(leaflet_df_pres) %>%
  setView(-68.5, leaflet_df_pres$lat, zoom = 7) %>%
  addTiles() %>%
  addMarkers(~as.numeric(lon), ~as.numeric(lat), label = ~location,
             labelOptions = labelOptions(noHide = TRUE, textsize = "16px"))
```
:::
:::

## Visualization of full time series

```{python}
fig = go.Figure([
    go.Scatter(x = windspeed_100m_df.index, y = windspeed_100m_df['windspeed_100m'], 
              mode = 'lines', legendrank = 1, 
              name = 'hourly', line=dict(color='#447099', width=0.75)),
    go.Scatter(x = moving_averages_24hr.index, y = moving_averages_24hr['windspeed_100m'], 
              mode = 'lines', legendrank = 1,
              name = '24 hour avg', line=dict(color='#4eb265', width=1), visible='legendonly'),
    go.Scatter(x = moving_averages_10day.index, y = moving_averages_10day['windspeed_100m'], 
              mode = 'lines', legendrank = 1, 
              name = '10 day avg', line=dict(color='red', width=1), visible='legendonly'),
    go.Scatter(x = moving_averages_30day.index, y = moving_averages_30day['windspeed_100m'], 
              mode = 'lines', legendrank = 1, 
              name = '30 day avg', line=dict(color='#e8601c', width=3), visible='legendonly')
])

fig.update_layout(
    width=1050,
    margin=dict(l=20, r=20, t=30, b=20),
    paper_bgcolor="#FFFFFF",
    plot_bgcolor='#f5f5f5',
    yaxis=dict(
        title_text="windspeed (m/s)",
        titlefont=dict(size=16)),
    title={
        'text': "Hourly Wind Speed",
        'y':0.99,
        'x':0.5,
        'xanchor': 'center',
        'yanchor': 'top'}
)
```

## Diurnal and Monthly Variability {.smaller}

-   Both wind speed and electricity demands fluctuate throughout the day and seasonally
-   Wind and electricity patterns may not match
-   Unlike water resources, electricity can be challenging to store during low demands
-   When selecting sites for utility scale wind power it is important to have adequate wind speeds at the same time as peak electricity demands
-   Statistics such as the interquartile range and standard deviation can help quantify the spread of data

## Diurnal and Monthly Variability {.smaller}

::: columns
::: {.column width="50%"}
**Hourly average**

```{python}
fig = go.Figure([
    go.Scatter(name = 'mean', y = hourly_avg['mean'], x = hourly_avg['hour'], mode = 'lines',
              line = dict(color = "blue", width = 4),
              error_y = dict(type = 'data', array = hourly_avg['std'], visible = True)),
    go.Scatter(
        name = 'IQR 75', y = hourly_avg['quantile75'], x = hourly_avg['hour'],
        mode='lines',
        marker=dict(color="#444"),
        line=dict(width=0),
        #legendgroup = 'IQR',
        showlegend = False
    ),
    # Create IQR 25 fill color
    go.Scatter(
        name='IQR', y = hourly_avg['quantile25'], x = hourly_avg['hour'],
        marker=dict(color="#444"),
        line=dict(width=0),
        mode='lines',
        fillcolor='rgba(68, 68, 68, 0.3)',
        fill='tonexty', # fill to next y
        legendgroup = 'IQR',
        showlegend = True
    )
])
fig.update_layout(
    width=540,
    height=400,
    xaxis=dict(
        title_text="hour (UTC)",
        titlefont=dict(size=16),
        dtick = 2),
    yaxis=dict(
        title_text="windspeed (m/s)",
        titlefont=dict(size=16)),
    title={
        'text': "Average Hourly Wind Speed for the Year 2012",
        'y':0.99,
        'x':0.5,
        'xanchor': 'center',
        'yanchor': 'top'},
    margin=dict(l=20, r=20, t=30, b=20),
    paper_bgcolor="#FFFFFF",
    plot_bgcolor='#f5f5f5'
)
```

:::

::: {.column width="50%"}
**Monthly average**

```{python}
#| code-fold: true
#| code-summary: "hourly averages with IQR and std"
fig = go.Figure([
    go.Scatter(name = 'mean', y = monthly_avg['mean'], x = monthly_avg['month'], 
              mode = 'lines', line = dict(color = "blue", width = 4),
              error_y = dict(type = 'data', array = monthly_avg['std'], visible = True)),
    go.Scatter(
        name = 'IQR 75', y = monthly_avg['quantile75'], x = monthly_avg['month'],
        mode='lines', marker=dict(color="#444"), line=dict(width=0),
        showlegend = False
    ),

    # Create IQR 25 fill color
    go.Scatter(
        name='IQR', y = monthly_avg['quantile25'], x = monthly_avg['month'],
        marker=dict(color="#444"), line=dict(width=0), mode='lines',
        fillcolor='rgba(68, 68, 68, 0.3)',
        fill='tonexty', # fill to next y
        legendgroup = 'IQR',
        showlegend = True)
])
fig.update_layout(
     width=540,
    height=400,
    xaxis=dict(
        title_text="month",
        titlefont=dict(size=16),
        dtick = 1),
    yaxis=dict(
        title_text="windspeed (m/s)",
        titlefont=dict(size=16)),
    title={
        'text': "Average Monthly Wind Speed for the Year 2012",
        'y':0.99,
        'x':0.5,
        'xanchor': 'center',
        'yanchor': 'top'},
    margin=dict(l=20, r=20, t=30, b=20),
    paper_bgcolor="#FFFFFF",
    plot_bgcolor='#f5f5f5'
)
```

:::
:::

## Diurnal and Monthly Variability {.smaller}

```{python}
#| include: false
fig = go.Figure([
    go.Scatter(y = hourly_avg_by_month['1'], x = hourly_avg_by_month.index, 
              mode = 'lines', legendrank = 1, 
              name = 'January', line=dict(color='#DC050C', width=2)),
    go.Scatter(y = hourly_avg_by_month['2'], x = hourly_avg_by_month.index,
              mode = 'lines+markers', legendrank = 2, 
              name = 'February', line=dict(color='#E8601c', width=2)),
    go.Scatter(y = hourly_avg_by_month['3'], x = hourly_avg_by_month.index, 
              mode = 'lines', legendrank = 3, 
              name = 'March', line=dict(color='#f4a736', width=2)),
    go.Scatter(y = hourly_avg_by_month['4'], x = hourly_avg_by_month.index, 
              mode = 'lines+markers', legendrank = 4, 
              name = 'April', line=dict(color='#f7f056', width=2)),
    go.Scatter(y = hourly_avg_by_month['5'], x = hourly_avg_by_month.index, 
              mode = 'lines', legendrank = 5, 
              name = 'May', line=dict(color='#cae0ab', width=2)),
    go.Scatter(y = hourly_avg_by_month['6'], x = hourly_avg_by_month.index, 
              mode = 'lines+markers', legendrank = 6, 
              name = 'June', line=dict(color='#4eb265', width=2)),
    go.Scatter(y = hourly_avg_by_month['7'], x = hourly_avg_by_month.index, 
              mode = 'lines', legendrank = 7, 
              name = 'July', line=dict(color='#7bafde', width=2)),
    go.Scatter(y = hourly_avg_by_month['8'], x = hourly_avg_by_month.index, 
              mode = 'lines+markers', legendrank = 8, 
              name = 'August', line=dict(color='#5289c7', width=2)),
    go.Scatter(y = hourly_avg_by_month['9'], x = hourly_avg_by_month.index, 
              mode = 'lines', legendrank = 9, 
              name = 'September', line=dict(color='#1965b0', width=2)),
    go.Scatter(y = hourly_avg_by_month['10'], x = hourly_avg_by_month.index, 
              mode = 'lines+markers', legendrank = 10, 
              name = 'October', line=dict(color='#882e72', width=2)),
    go.Scatter(y = hourly_avg_by_month['11'], x = hourly_avg_by_month.index, 
              mode = 'lines', legendrank = 11, 
              name = 'November', line=dict(color='#ae76a3', width=2)),
    go.Scatter(y = hourly_avg_by_month['12'], x = hourly_avg_by_month.index, 
              mode = 'lines+markers', legendrank = 12, 
              name = 'December', line=dict(color='#d1bbd7', width=2)),
    go.Scatter(name = 'annual mean', y = hourly_avg['mean'], x = hourly_avg['hour'], mode = 'lines',
              line = dict(color = "black", width = 5))

])

variables_to_hide = ['February', 'March', 'April', 'May', 'June', 'July',
                    'August', 'September', 'October', 'November', 'December']
fig.for_each_trace(lambda trace: trace.update(visible="legendonly") 
                   if trace.name in variables_to_hide else ())
```

```{python}
fig.update_layout(
    width=1050,
    height=500,
    xaxis=dict(
        title_text="hour (UTC)",
        titlefont=dict(size=16),
        dtick = 4),
    yaxis=dict(
        title_text="windspeed (m/s)",
        titlefont=dict(size=16)),
    title={
        'text': "Average Hourly Wind Speed by Month",
        'y':0.99,
        'x':0.5,
        'xanchor': 'center',
        'yanchor': 'top'},
    margin=dict(l=20, r=20, t=30, b=20),
    paper_bgcolor="#FFFFFF",
    plot_bgcolor='#f5f5f5'
)
```

## Diurnal and Monthly Variability {.smaller}

```{python}
heatmap_month = hourly_avg_by_month.columns.tolist()
heatmap_hour = hourly_avg_by_month.index.tolist()
heatmap_windspeed = hourly_avg_by_month.values.tolist()

trace = go.Heatmap(
   x = heatmap_month,
   y = heatmap_hour,
   z = heatmap_windspeed,
   type = 'heatmap',
   #colorscale = [(0,"blue"), (1,"red")],
   colorscale = 'mint',
   colorbar=dict(title='Wind Speed (m/s)')
)
data = [trace]
fig = go.Figure(data = data)

fig.update_layout(
    width=1050,
    height=550,
    xaxis=dict(
        #title_text="month",
        titlefont=dict(size=16),
        #dtick = 1,
        tickmode = 'array',
        # Set tick intervals to correspond with months
        tickvals = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],
        ticktext = ['January', 'February', 'March', 'April', 
                    'May', 'June', 'July', 'August', 
                    'September', 'October', 'November', 'December'],
        tickfont = dict(size=16)),
    yaxis=dict(
        title_text="hour (UTC)",
        titlefont=dict(size=16),
        dtick = 1,
        tickfont = dict(size=16)),
    title={
        'text': "Average Wind Speed by Month and Hour",
        'y':0.99,
        'x':0.5,
        'xanchor': 'center',
        'yanchor': 'top'},
    margin=dict(l=20, r=20, t=30, b=20),
)
```

## Standard Deviation {.smaller}

```{python}
std_heatmap_month = hourly_std_by_month.columns.tolist()
std_heatmap_hour = hourly_std_by_month.index.tolist()
std_heatmap_windspeed = hourly_std_by_month.values.tolist()

trace = go.Heatmap(
   x = std_heatmap_month,
   y = std_heatmap_hour,
   z = std_heatmap_windspeed,
   type = 'heatmap',
   colorscale = 'Blues',
   colorbar=dict(title='Standard Deviation (m/s)')
)
data = [trace]
fig = go.Figure(data = data)

fig.update_layout(
    width=1050,
    height=550,
    xaxis=dict(
        title_text="month",
        titlefont=dict(size=16),
        #dtick = 1,
        tickmode = 'array',
        # Set tick intervals to correspond with months
        tickvals = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],
        ticktext = ['January', 'February', 'March', 'April', 
                    'May', 'June', 'July', 'August', 
                    'September', 'October', 'November', 'December'],
        tickfont = dict(size=16)),
    yaxis=dict(
        title_text="hour (UTC)",
        titlefont=dict(size=16),
        dtick = 1,
        tickfont = dict(size=16)),
    title={
        'text': "Wind Speed Standard Deviation by Month and Hour",
        'y':0.99,
        'x':0.5,
        'xanchor': 'center',
        'yanchor': 'top'},
    margin=dict(l=20, r=20, t=30, b=20),
)
```

## Statistical Summary {.smaller}

|                 | min wind speed (m/s) | max wind speed (m/s) |
|-----------------|:--------------------:|:--------------------:|
| hourly          |         0.1          |         36.7         |
| hourly average  |         10.7         |         12.5         |
| monthly average |         8.8          |         15.8         |
|                 |                      |                      |

<br> <br>

+-------------+--------------------------+----------------------+-----------------------------+-----------------------------+
|             | **smallest variability** | greatest variability | smallest average wind speed | greatest average wind speed |
+=============+:========================:+:====================:+:===========================:+:===========================:+
| monthly     | July                     | November             | August                      | February                    |
+-------------+--------------------------+----------------------+-----------------------------+-----------------------------+
| hourly      | 19                       | 13                   | 16                          | 1                           |
+-------------+--------------------------+----------------------+-----------------------------+-----------------------------+
|             |                          |                      |                             |                             |
+-------------+--------------------------+----------------------+-----------------------------+-----------------------------+

## Conclusions {.smaller}

-   Greatest monthly variability in **November**
-   Smallest monthly variability in **July**
-   Highest wind speeds in **February**
-   Lowest wind speeds in **August**
-   Slower wind speeds mid-day than at night
-   Based on the seasonal variability, this site would be better at meeting high winter demands than summer demands
-   This site may not be ideal for meeting all daytime demands.

## Expanded Geographic Scale {.smaller}

Created a parameterized [report](https://marierivers.github.io/wind_resource_temporal_variability/report.pdf) using [Quarto](https://quarto.org/docs/computations/parameters.html) as a tool to allow users to generate summary reports based on specified inputs from CLI or with render function for multiple sites.

::: columns
::: {.column width="55%"}
**Modify by specifying parameters for:**

-   site name
-   site latitude
-   site longitude
-   start date
-   end date
-   turbine cut-in speed
-   turbine cut-out speed
-   required annual average wind speed
:::

::: {.column width="45%"}
**Default parameters are:**

-   start date: '2012-01-01'
-   end date: '2013-01-01'
-   cut-in speed: 3.6 m/s
-   cut-out speed: 24.6 m/s
-   required annual avg speed: 5.8 m/s
:::
:::

## Expanded Geographic Scale {.smaller}

In the CLI the example code below renders a report for NYC in 2010 using default values for cut-in speed, cut-out speed, and required annual average wind speed.

> quarto render report.qmd -P site_name:"New York City" -P site_lat:40.7128 -P site_lon:-74.0059 -P start_date:2010-01-01 -P end_date:2011-01-01 \--output new_york_city_report.pdf (\>)

This function generates multiples reports from a dataframe of parameters for different sites.

```{r}
#| eval: false
#| echo: true
render_fun <- function(param_df){
  quarto::quarto_render(
    input = "report.qmd",
    execute_params = list(site_name = param_df$site_name,
                          site_lat = param_df$site_lat,
                          site_lon = param_df$site_lon,
                          start_date = param_df$start_date,
                          end_date = param_df$end_date),
    output_file = glue::glue("{param_df$site_name}-report.pdf"))}

param_list <- split(report_parameters, seq(nrow(report_parameters))) %>% 
  purrr::walk(render_fun)
```

## Report {.smaller}

For the specified site, the report answers the following questions:

-   Is the annual average wind speed at least 13 mph (5.8 m/s)? [^1]
-   How often the wind is below the cut-in speed of 8 mph (3.6 m/s)? [^2]
-   How often the wind exceed the cut-out speed of 55 mph (24.6 m/s)?

[^1]: The U.S. Energy Information Administration recommends an annual average wind speed of at least 9 mph (4 m/s) for small wind turbines and 13 mph (5.8 m/s) for utility-scale turbines. <https://www.eia.gov/energyexplained/wind/where-wind-power-is-harnessed.php#:~:text=Good%20places%20for%20wind%20turbines,)%20for%20utility%2Dscale%20turbines.>

[^2]: The Office of Energy Efficiency & Renewable Energy notes a typical cut-in speed of 6 to 9 mpg and cut-out speed of 55 mph. <https://www.energy.gov/eere/articles/how-do-wind-turbines-survive-severe-storms>

[University of Delaware](University%20of%20Delaware-report.pdf)

[UMass Amherst](Umass%20Amherst-report.pdf)

[UC Santa Barbara](UC%20Santa%20Barbara-report.pdf)

[NREL - Golden, CO](Golden,%20CO-report.pdf)

## Citations {.smaller}

Draxl, C., B.M. Hodge, A. Clifton, and J. McCaa. 2015. [Overview and Meteorological Validation of the Wind Integration National Dataset Toolkit (Technical Report](https://www.nrel.gov/docs/fy15osti/61740.pdf), NREL/TP-5000-61740). Golden, CO: National Renewable Energy Laboratory.

Draxl, C., B.M. Hodge, A. Clifton, and J. McCaa. 2015. "[The Wind Integration National Dataset (WIND) Toolkit.](https://www.sciencedirect.com/science/article/pii/S0306261915004237?via%3Dihub)" Applied Energy 151: 355366.

King, J., A. Clifton, and B.M. Hodge. 2014. [Validation of Power Output for the WIND Toolkit](https://www.nrel.gov/docs/fy14osti/61714.pdf) (Technical Report, NREL/TP-5D00-61714). Golden, CO: National Renewable Energy Laboratory.

<https://www.eia.gov/electricity/gridmonitor/dashboard/electric_overview/US48/US48>

<https://www.eia.gov/energyexplained/wind/where-wind-power-is-harnessed.php#:~:text=Good%20places%20for%20wind%20turbines,)%20for%20utility%2Dscale%20turbines.>

# Snow Today {background-color="#447099"}

Improving usability of snow data through web based visualizations and tutorials

##  {background-image="images/snow_collage.jpg"}

::: {style="text-align: center"}
[Knowing the spatial extent of snow cover is critical for water management and winter recreation. Climate change will affect the variability of frozen water resources.]{.big_centered_text .absolute top="40%" width="100%"}
:::

::: footer
<https://shiny.snow.ucsb.edu/snow_today_shiny_app/>
:::

##  {background-image="images/field_visit.jpg" background-size="100%"}

[Snow Science: UCSB CUES Field Station Site Visit]{.big_centered_text .absolute top="42%" width="100%"}

::: footer
<https://shiny.snow.ucsb.edu/snow_today_shiny_app/>
:::

##  {background-image="images/albedo_example.jpg" background-size="95%"}

::: footer
<https://shiny.snow.ucsb.edu/snow_today_shiny_app/>
:::

## Albedo Importance {.smaller}

::: columns
::: {.column width="60%"}
-   Regulates the Earth's temperature by reflecting solar radiation <br> <br>
-   Influences rate of snow melt <br> <br>
-   Particularly important in the Western US <br> <br>
-   Accurate estimates critical for climate models and predicting water storage
:::

::: {.column width="40%"}
![](images/dirty_snow1.jpg){.absolute top="50" right="0" width="400"}

![](images/dirty_snow2.png){.absolute bottom="50" right="0" width="400"}
:::
:::

::: footer
<https://shiny.snow.ucsb.edu/snow_today_shiny_app/>
:::

## Snow Today {.smaller}

::: columns
::: {.column width="60%"}
-   Scientific analysis website that provides data on snow conditions from satellite and surface measurements <br> <br>
-   Used by scientists, water managers, and outdoor enthusiasts for snow observations <br> <br>
-   Spatial products offered include measures of snow cover extent and albedo
:::

::: {.column width="40%"}
![](images/existing_snow_today_website.png){width="505"}
:::
:::

::: footer
<https://shiny.snow.ucsb.edu/snow_today_shiny_app/>
:::

## Snow Today: Usability {.smaller}

::: columns
::: {.column width="40%"}
![](images/snowy_mountain.png){width="366"}
:::

::: {.column width="60%"}
Snow Today can be [hard to navigate]{.yellow} for new users unfamiliar with the website's layout

Visualizations are of current snow conditions, and have [limited customization options]{.yellow}

Snow cover and albedo files are [hard to find]{.yellow}

Data format may be [challengingfor new users]{.yellow}

Snow metadata is stored in a non-standardized format which is [difficult for some software to interpret]{.yellow} the data

Users may have [trouble processing and analyzing snow data]{.yellow} without the help
:::
:::

::: footer
<https://shiny.snow.ucsb.edu/snow_today_shiny_app/>
:::

## Snow Today: Visualizations {.smaller}

::: columns
::: {.column width="50%"}
![](images/existing_vis1.png){width="564"}
:::

::: {.column width="50%"}
![](images/existing_vis2.png){width="562"}
:::
:::

::: footer
<https://shiny.snow.ucsb.edu/snow_today_shiny_app/>
:::

## Objectives {.smaller}

::: columns
::: {.column width="30%"}
![](images/snowy_tree.jpg){width="338"}
:::

::: {.column width="70%"}
**Create an open source workflow for processing and visualizing snow data**

::: {.fragment fragment-index="1"}
::: {.fragment .highlight-blue fragment-index="4"}
1.  Provide recommendations for the Snow Today website
:::
:::

::: {.fragment fragment-index="2"}
2.  Create interactive visualizations
:::

::: {.fragment fragment-index="3"}
3.  Improve data usability through tutorials in Python
:::
:::
:::

::: footer
<https://shiny.snow.ucsb.edu/snow_today_shiny_app/>
:::

## Website Recommendations {.smaller}

**website architecture**

![](images/website_architechture1.png){width="100%"}

::: footer
<https://shiny.snow.ucsb.edu/snow_today_shiny_app/>
:::

## Website Recommendations {.smaller}

::: columns
::: {.column width="50%"}
![](images/wireframe1.png){width="1515"}
:::

::: {.column width="50%"}
![](images/wireframe2.png)
:::
:::

::: footer
<https://shiny.snow.ucsb.edu/snow_today_shiny_app/>
:::

## Objectives {.smaller}

::: columns
::: {.column width="30%"}
![](images/snowy_tree.jpg){width="338"}
:::

::: {.column width="70%"}
**Create an open source workflow for processing and visualizing snow data**

1.  Provide recommendations for the Snow Today website

[2. Create interactive visualizations]{.bright_blue}

3.  Improve data usability through tutorials in Python
:::
:::

::: footer
<https://shiny.snow.ucsb.edu/snow_today_shiny_app/>
:::

## Interactive Visualizations {.smaller}

::: columns
::: {.column width="40%"}
**Prototype Web Application**

<br> <br> Daily maps of snow cover and albedo for any selected date
:::

::: {.column width="60%"}
![](images/daily_snow.gif){.absolute right="0%"}
:::
:::

::: footer
<https://shiny.snow.ucsb.edu/snow_today_shiny_app/>
:::

## Interactive Visualizations {.smaller}

::: columns
::: {.column width="40%"}
**Monthly Average and Anomaly**

<br> <br> Users can select a specific month, water year, and variable to view averages on anomalies.
:::

::: {.column width="60%"}
![](images/monthly_snow.gif){.absolute right="0%"}
:::
:::

::: footer
<https://shiny.snow.ucsb.edu/snow_today_shiny_app/>
:::

## Interactive Visualizations {.smaller}

**Annual Comparisons**

![](images/annual_snow.gif)

::: footer
<https://shiny.snow.ucsb.edu/snow_today_shiny_app/>
:::

## Interactive Visualizations {.smaller}

```{python}
#| include: false
snow_cover_df = pd.read_csv('data/snow_cover_df.csv')
snow_cover_df = snow_cover_df.fillna(0)

# Create empty list to input with for loop
IQR_25 = []
IQR_75 = []
IQR_50 = []
days = []
for i in range(len(snow_cover_df)): 
    #Takes the IQR of each day (25, 50, 75)
    Q1 = np.percentile(snow_cover_df.iloc[i], 25)
    Q2 = np.percentile(snow_cover_df.iloc[i], 50)
    Q3 = np.percentile(snow_cover_df.iloc[i], 75)
    #appends list with IQR outputs
    IQR_25.append(Q1)
    IQR_50.append(Q2)
    IQR_75.append(Q3)
    #Creates day list to append dataset with
    days.append(i + 1)
    
# Next, need to create a single column of mean values. 
snow_cover_df['Average Snow Cover'] = snow_cover_df.mean(axis = 1)

#Appends list for loop lists
snow_cover_df['IQR_25'] = IQR_25
snow_cover_df['IQR_75'] = IQR_75
snow_cover_df['IQR_50'] = IQR_50
snow_cover_df['days'] = days

month_day = [31, 30, 31, 31, 28, 31, 30, 31, 30, 31, 31, 30]
new_list = []

j = 0 
for i in range(0,len(month_day)):
    j+=month_day[i]
    new_list.append(j)
    
# Create a list of years to graph. legend rank allows lets you order where the lines are located on the chart. 
for i in range(len(snow_cover_df)):
    print("""go.Scatter("""
        """name = '""" + str(i + 2001) + """', """
        """y = snow_cover_df['"""+ str(i + 2001) + """'], x = snow_cover_df['days'], """
        """mode = 'lines', legendrank = """ + str(19-i) + """),"""
    )
```

```{python}
#| include: false
#Plot the figure. 
fig = go.Figure([

#create median line
go.Scatter(
    name = 'Median',
    y = snow_cover_df['IQR_50'],
    x = snow_cover_df['days'],
    mode = 'lines',
    legendgroup = 'IQR',
    legendgrouptitle_text="<b>Interquartile Range</b>",
    legendrank = 20,
    line=dict(color='rgb(31, 119, 180)'),
),
#Create IQR 75 line
go.Scatter(
        name = 'IQR 75', y = snow_cover_df['IQR_75'], x = snow_cover_df['days'],
        mode='lines', marker=dict(color="#444"), line=dict(width=0),
        legendgroup = 'IQR', showlegend = False # Here we 'hide' the name from appearing on the legend since it's lumped in with the legendgroup 'IQR'
    ),
    #Create IQR 25 fill color
    go.Scatter(
        name='IQR 25', y = snow_cover_df['IQR_25'], x = snow_cover_df['days'],
        marker=dict(color="#444"), line=dict(width=0),  mode='lines',
        fillcolor='rgba(68, 68, 68, 0.3)', fill='tonexty',
        legendgroup = 'IQR', showlegend = False
    ),
    #Create mean line
    go.Scatter(
        name = 'Average Snow Cover',  y = snow_cover_df['Average Snow Cover'], x = snow_cover_df['days'],
        mode = 'lines', legendgroup = 'Average',
        legendgrouptitle_text = '<b>Average</b>', legendrank = 21
    ),
#Create lines for each respective year
go.Scatter(name = '2001', y = snow_cover_df['2001'], x = snow_cover_df['days'], mode = 'lines', legendrank = 19),
go.Scatter(name = '2002', y = snow_cover_df['2002'], x = snow_cover_df['days'], mode = 'lines', legendrank = 18),
go.Scatter(name = '2003', y = snow_cover_df['2003'], x = snow_cover_df['days'], mode = 'lines', legendrank = 17),
go.Scatter(name = '2004', y = snow_cover_df['2004'], x = snow_cover_df['days'], mode = 'lines', legendrank = 16),
go.Scatter(name = '2005', y = snow_cover_df['2005'], x = snow_cover_df['days'], mode = 'lines', legendrank = 15),
go.Scatter(name = '2006', y = snow_cover_df['2006'], x = snow_cover_df['days'], mode = 'lines', legendrank = 14),
go.Scatter(name = '2007', y = snow_cover_df['2007'], x = snow_cover_df['days'], mode = 'lines', legendrank = 13),
go.Scatter(name = '2008', y = snow_cover_df['2008'], x = snow_cover_df['days'], mode = 'lines', legendrank = 12),
go.Scatter(name = '2009', y = snow_cover_df['2009'], x = snow_cover_df['days'], mode = 'lines', legendrank = 11),
go.Scatter(name = '2010', y = snow_cover_df['2010'], x = snow_cover_df['days'], mode = 'lines', legendrank = 10),
go.Scatter(name = '2011', y = snow_cover_df['2011'], x = snow_cover_df['days'], mode = 'lines', legendrank = 9),
go.Scatter(name = '2012', y = snow_cover_df['2012'], x = snow_cover_df['days'], mode = 'lines', legendrank = 8),
go.Scatter(name = '2013', y = snow_cover_df['2013'], x = snow_cover_df['days'], mode = 'lines', legendrank = 7),
go.Scatter(name = '2014', y = snow_cover_df['2014'], x = snow_cover_df['days'], mode = 'lines', legendrank = 6),
go.Scatter(name = '2015', y = snow_cover_df['2015'], x = snow_cover_df['days'], mode = 'lines', legendrank = 5),
go.Scatter(name = '2016', y = snow_cover_df['2016'], x = snow_cover_df['days'], mode = 'lines', legendrank = 4),
go.Scatter(name = '2017', y = snow_cover_df['2017'], x = snow_cover_df['days'], mode = 'lines', legendrank = 3),
go.Scatter(name = '2018', y = snow_cover_df['2018'], x = snow_cover_df['days'], mode = 'lines', legendrank = 2),
go.Scatter(name = '2019', y = snow_cover_df['2019'], x = snow_cover_df['days'], mode = 'lines', legendrank = 1)

])
# Can change default "off" variables. Right now, the only variable visible is year_2019 and IQR
variables_to_hide = [
'2001', '2002', '2003',
'2004', '2005', '2006', '2007',
'2008', '2009', '2010', '2011',
'2012', '2013', '2014', '2015',
# '2016', '2017', '2018', '2019',
'Average Snow Cover', 'Median']
fig.for_each_trace(lambda trace: trace.update(visible="legendonly") 
                   if trace.name in variables_to_hide else ())
fig.update_layout(
    title = "<b> Annual Snow Cover Area: Sierra Nevada Region </b> <br> <sup>2001-2019</sup></br>",
    legend_title="<b>Year</b>",
    autosize=False,
    width=1050,
    height=550,
    template = 'none',
    font=dict(
        size=16),
xaxis = dict(
        tickmode = 'array',
        tickvals = [1, 31, 61, 92, 123, 151, 182, 212, 243, 273, 304, 335, 365],
        ticktext = ['<b>October</b>', '<b>November</b>', '<b>December</b>', '<b>January</b>', '<b>February</b>', '<b>March</b>', '<b>April</b>', '<b>May</b>', 
        '<b>June</b>', '<b>July', '<b>August</b>', "<b>September</b>", "<b>October</b>"],
        tickfont = dict(size=12))
)
fig.update_xaxes(title_text = "", gridcolor = 'lightgrey', gridwidth = 0.1)
```

```{python}
fig.update_yaxes(title_text = "<b> Area (Thousands of Square Kilometers) </b>", 
    title_font = {"size": 15}, gridcolor = 'lightgrey', gridwidth = 0.1)
```

## Objectives {.smaller}

::: columns
::: {.column width="30%"}
![](images/snowy_tree.jpg){width="338"}
:::

::: {.column width="70%"}
**Create an open source workflow for processing and visualizing snow data**

1.  Provide recommendations for the Snow Today website

2.  Create interactive visualizations

[3. Improve data usability through tutorials in Python]{.bright_blue}
:::
:::

::: footer
<https://shiny.snow.ucsb.edu/snow_today_shiny_app/>
:::

## Tutorials {.smaller}

::: columns
::: {.column width="45%"}
<br> [1. Download and Explore Datasets]{.text_block_blue} <br> <br> <br> <br> [2. Process and Format Data]{.text_block_green} <br> <br> <br> <br> [3. Analyze and Visualize Snow Data]{.text_block_orange}
:::

::: {.column width="55%"}
![](images/tuturial_example.gif){width="624"}
:::
:::

::: footer
<https://shiny.snow.ucsb.edu/snow_today_shiny_app/>
:::

## Tutorials {.smaller}

::: columns
::: {.column width="45%"}
<br> [1. Download and Explore Datasets]{.text_block_blue} <br> <br> <br> <br> [2. Process and Format Data]{.text_block_green_lt} <br> <br> <br> <br> [3. Analyze and Visualize Snow Data]{.text_block_orange_lt}
:::

::: {.column width="55%"}
-   Download snow cover and albedo datasets
-   Open datasets and view metadata
-   Create basic visualizations of each dataset
:::
:::

::: footer
<https://shiny.snow.ucsb.edu/snow_today_shiny_app/>
:::

## Tutorials {.smaller}

::: columns
::: {.column width="45%"}
<br> [1. Download and Explore Datasets]{.text_block_blue_lt} <br> <br> <br> <br> [2. Process and Format Data]{.text_block_green} <br> <br> <br> <br> [3. Analyze and Visualize Snow Data]{.text_block_orange_lt}
:::

::: {.column width="55%"}
-   Process and subset datasets
-   Calculate monthly and yearly snow cover and albedo averages and anomalies
-   Create interactive maps of processed data
-   Convert processed data to GeoTiff and NetCDF formats
:::
:::

::: footer
<https://shiny.snow.ucsb.edu/snow_today_shiny_app/>
:::

## Tutorials {.smaller}

::: columns
::: {.column width="45%"}
<br> [1. Download and Explore Datasets]{.text_block_blue_lt} <br> <br> <br> <br> [2. Process and Format Data]{.text_block_green_lt} <br> <br> <br> <br> [3. Analyze and Visualize Snow Data]{.text_block_orange}
:::

::: {.column width="55%"}
-   Calcualte total snow cover area and average albedo for entire spatial domain
-   Perform basic statistical analysis of datasets
-   Develop interactive charts to compare snow cover area and albedo percentages for each water year
:::
:::

::: footer
<https://shiny.snow.ucsb.edu/snow_today_shiny_app/>
:::

## Contributions

![](images/contributions_png.png)

##  {background-image="images/snow_and_water.jpg" background-size="100%"}

[As water resources become harder to manage due to climate change, implementing these tools will open a valuable dataset to a wider audience]{.big_centered_text .absolute top="0%" width="100%"}

::: footer
<https://shiny.snow.ucsb.edu/snow_today_shiny_app/>
:::

## Acknowledgements {.smaller}

**Faculty Advisors**

Sam Stevenson, UCSB Bren School

Allison Horst, UCSB Bren School

**Clients**

Timbo Stillinger, UCSB Earth Research Institute

Ned Bair, UCSB Earth Research Institute

Karl Rittger, CU Boulder Institute of Arctic & Alpine Research

**External Advisors**

James Frew, UCSB Bren School

Niklas Griessbaum, UCSB Bren School

Kat Le, UCSB Bren School

Michael Colee, UCSB Geography & Earth Research Institute

**Bren School Faculty and Staff and the MEDS 2022 cohort**

::: footer
<https://shiny.snow.ucsb.edu/snow_today_shiny_app/>
:::

# Houston Power Outages {background-color="#447099"}

A Geospatial and Statistical Analysis

## Background {.smaller}

::: columns
::: {.column width="55%"}
In February 2021 the Houston, TX metropolitan area experienced wide scale power outages due to electrical infrastructure failure during winter storms and extreme cold temperatures
:::

::: {.column width="40%"}
![](images/houston_street_map.png){.absolute top="0" right="0" width="450"}
:::
:::

<br> <br> <br> <br> <br> <br> <br> [1.4 million customers without power]{.fragment .text_block}

::: footer
<https://marierivers.github.io/blackout_analysis/>
:::

## Approach {.smaller}

**Use geospatial and statistical methods to quantify:**

1.  Number of residential homes without power

2.  Socioeconomic differences of areas with and without power

**...by using data from:**

1.  Satellite imagery of nighttime lights

2.  OpenStreetMaps roadways and buidings

3.  Census tract level race, age and income variables

::: footer
<https://marierivers.github.io/blackout_analysis/>
:::

## Project overview {.smaller}

::: columns
::: {.column width="40%"}
::: {.fragment fragment-index="1"}
1.  Load satellite imagery
:::

::: {.fragment fragment-index="2"}
2.  Create blackout mask
:::

::: {.fragment fragment-index="3"}
3.  Identify residential buildings within blackout area
:::

::: {.fragment fragment-index="4"}
4.  Spatially join census data to blackout areas
:::
:::
:::

::: {.fragment fragment-index="1"}
![](images/step1_satellite_imagery.png){.absolute top="70" right="300"}
:::

::: {.fragment fragment-index="2"}
![](images/step2_blackout_mask.png){.absolute top="70" right="0"}
:::

::: {.fragment fragment-index="3"}
![](images/step3_buildings.png){.absolute bottom="50" right="300" width="295"}
:::

::: {.fragment fragment-index="4"}
![](images/step4_spatial_join.png){.absolute bottom="50" right="0"}
:::

<br> <br> [+144,000 households without power]{.fragment .text_block_small fragment-index="5"}

::: footer
<https://marierivers.github.io/blackout_analysis/>
:::

::: footer
<https://marierivers.github.io/blackout_analysis/>
:::

## 

::: columns
::: {.column width="50%"}
**Percent white**

::: r-stack
![](images/pct_white_map.png){.fragment width="425" fragment-index="1"}

![](images/pct_white_blackout_map.png){.fragment fragment-index="2" width="425"}
:::
:::

::: {.column width="50%"}
**Percent bipoc**

::: r-stack
![](images/pct_bipoc_map.png){.fragment fragment-index="1" width="425"}

![](images/pct_bipoc_blackout_map.png){.fragment fragment-index="2" width="425"}
:::
:::
:::

::: footer
<https://marierivers.github.io/blackout_analysis/>
:::

## Statistical Analysis {.smaller}

Linear regression models of percent of households without power vs. census variables

+-----------------+----------------------------------+
| [Race]{.blue}   | percent white                    |
|                 |                                  |
|                 | percent black                    |
|                 |                                  |
|                 | percent Native American          |
|                 |                                  |
|                 | percent Asian                    |
|                 |                                  |
|                 | percent Hispanic / Latino        |
+-----------------+----------------------------------+
| [Age]{.blue}    | 65 and older                     |
|                 |                                  |
|                 | children under 18                |
+-----------------+----------------------------------+
| [Income]{.blue} | percent households below poverty |
|                 |                                  |
|                 | median income                    |
+-----------------+----------------------------------+

::: footer
<https://marierivers.github.io/blackout_analysis/>
:::

## Statistical Analysis {.smaller}

```{r}
#| include: false
blackout_census_data <- read_csv("data/census_tract_blackout_data_df.csv")
```

```{r}
# linear regression model
model_pct_white <- lm(data = blackout_census_data, pct_houses_that_lost_power ~ pct_white)

#plot
plot_model_pct_white <- ggplot(data = blackout_census_data, aes(x = pct_white, y = pct_houses_that_lost_power)) +
  geom_point(size = 0.5) +
  geom_smooth(method = lm, formula = y~x, se = FALSE) +
  theme_classic() +
  labs(x = "% white", y = "% of houses that lost power",
       title = "Linear regression of % households without power vs. % population white")
plot_model_pct_white
```

::: footer
<https://marierivers.github.io/blackout_analysis/>
:::

## Statistical Analysis {.smaller}

```{r}
# linear regression model
model_pct_black <- lm(data = blackout_census_data, pct_houses_that_lost_power ~ pct_black)

# plot
plot_model_pct_black <- ggplot(data = blackout_census_data, aes(x = pct_black, y = pct_houses_that_lost_power)) +
  geom_point(size = 0.5) +
  geom_smooth(method = lm, formula = y~x, se = FALSE) +
  theme_classic() +
  labs(x = "% black", y = "% of houses that lost power",
       title = "Linear regression of % households without power vs. % population black")
plot_model_pct_black
```

::: footer
<https://marierivers.github.io/blackout_analysis/>
:::

## Results {.smaller}

::: r-stack
![](images/blackout_results1.png){.fragment}

![](images/blackout_results2.png){.fragment}

![](images/blackout_results3.png){.fragment}
:::

::: footer
<https://marierivers.github.io/blackout_analysis/>
:::

## Conclusions {.smaller}

::: columns
::: {.column width="55%"}
-   While race, age and income accounted for small portions of the overall variance in residential power outages, this analysis suggests some racial and economic inequality.

-   Electric utilities should evaluate infrastructure and asset management plans in areas with higher proportions of people of color and poverty

-   There is a need for more equitable responses to natural disasters
:::

::: {.column width="40%"}
![](images/Houston_bmhd_2021038_lrg_Feb_7_2021.jpg) image source[^3]
:::
:::

[^3]: https://appliedsciences.nasa.gov/our-impact/news/extreme-winter-weather-causes-us-blackouts

::: footer
<https://marierivers.github.io/blackout_analysis/>
:::
